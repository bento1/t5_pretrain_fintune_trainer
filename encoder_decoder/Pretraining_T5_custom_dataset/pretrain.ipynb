{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/torch/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/torch/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/torch/lib/python3.9/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_input_length': 512, 'max_output_length': 512, 'num_train_epochs': 100, 'output_dir': 't5_pretraining', 'train_batch_size': 2, 'learning_rate': 1e-05, 'model_name_or_path': 'lcw99/t5-base-korean-text-summary', 'tokenizer_name_or_path': 'lcw99/t5-base-korean-text-summary', 'freeze_encoder': False, 'freeze_embeds': False, 'weight_decay': 0.0, 'adam_epsilon': 1e-08, 'warmup_steps': 0, 'eval_batch_size': 2, 'gradient_accumulation_steps': 1, 'n_gpu': 1, 'resume_from_checkpoint': None, 'check_val_every_n_epoch': 2, 'n_val': 4, 'val_percent_check': 5, 'n_train': 250, 'n_test': -1, 'early_stop_callback': False, 'fp_16': False, 'opt_level': 'O1', 'max_grad_norm': 0.5, 'seed': 42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================================================================\n",
      "Layer (type:depth-idx)                                            Output Shape              Param #\n",
      "===================================================================================================================\n",
      "T5FineTuner                                                       [1, 2048, 768]            --\n",
      "├─T5ForConditionalGeneration: 1-1                                 [1, 2048, 768]            --\n",
      "│    └─T5Stack: 2-1                                               [1, 2048, 768]            123,629,184\n",
      "│    └─T5Stack: 2-2                                               --                        (recursive)\n",
      "│    │    └─Embedding: 3-1                                        [1, 2048, 768]            38,674,944\n",
      "│    └─T5Stack: 2-3                                               --                        (recursive)\n",
      "│    │    └─Dropout: 3-2                                          [1, 2048, 768]            --\n",
      "│    │    └─ModuleList: 3-3                                       --                        84,953,472\n",
      "│    │    └─T5LayerNorm: 3-4                                      [1, 2048, 768]            768\n",
      "│    │    └─Dropout: 3-5                                          [1, 2048, 768]            --\n",
      "│    └─T5Stack: 2-4                                               [1, 12, 1, 64]            38,674,944\n",
      "│    │    └─Embedding: 3-6                                        [1, 1, 768]               (recursive)\n",
      "│    │    └─Dropout: 3-7                                          [1, 1, 768]               --\n",
      "│    │    └─ModuleList: 3-8                                       --                        113,274,240\n",
      "│    │    └─T5LayerNorm: 3-9                                      [1, 1, 768]               768\n",
      "│    │    └─Dropout: 3-10                                         [1, 1, 768]               --\n",
      "│    └─Linear: 2-5                                                [1, 1, 50358]             38,674,944\n",
      "===================================================================================================================\n",
      "Total params: 437,883,264\n",
      "Trainable params: 437,883,264\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 315.04\n",
      "===================================================================================================================\n",
      "Input size (MB): 0.03\n",
      "Forward/backward pass size (MB): 2593.63\n",
      "Params size (MB): 1102.32\n",
      "Estimated Total Size (MB): 3695.98\n",
      "===================================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/torch/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:615: UserWarning: Checkpoint directory /Users/dongunyun/study/datascience/encoder_decoder/Pretraining_T5_custom_dataset/t5_pretraining exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 275 M \n",
      "-----------------------------------------------------\n",
      "275 M     Trainable params\n",
      "0         Non-trainable params\n",
      "275 M     Total params\n",
      "1,102.317 Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf82ce15a99546febce152f0764adac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dongunyun/study/datascience/encoder_decoder/Pretraining_T5_custom_dataset/pretrain.py:419: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  ds = ds.append(ds2)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/torch/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:480: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/torch/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/torch/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/torch/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:396: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/torch/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:422: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a transporter accomplishing  expressed in inner  or deoxycorticosterone (DOCA).  tissues. METHODS: The  utilized RT-qPCR and  and protein abundance  Slc26a4 in murine  and following subcutaneous  100 mg/kg DOCA.', 't(11;15)(q23;q15) in a  carcinogenesis involving multiple  to germline mutations  in the CHEK2  cancer at age  also developed early-onset  bilateral breast cancer,  cell line revealed  t(11;15)(q23;q15). This translocation  in a primary  patient or her  although a nephew']\n",
      "['SLC26A4 is a disorder of pendrin expression into kidney and heart tissues.', 'LFS is characterized by early-onset tumor types and shows autosomal dominant inheritance for a Li-Fraumeni syndrome family']\n",
      "['autosomal dominant, human  a key role  a vertebrate system  genetic screens and  a forward genetic  some LFS mutations,  loss-of-function allele with  in vivo. Additionally,  is evolutionarily conserved  novel modifier genes  therapeutic agents for', 'been established as  activity is limited  acquired resistance which  of continued treatment.  selection of the  gene mutation, transdifferentiation  binding and inhibiting  common activating EGFR  but was also  showed promising efficacy  review the current']\n",
      "['LFS is a highly penetrant cancer predisposition of the tumor suppressor.', 'EGFR T790M mutation in approximately 50% of cases and MET gene amplification, PIK3CA resistance']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dongunyun/study/datascience/encoder_decoder/Pretraining_T5_custom_dataset/pretrain.py:419: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  ds = ds.append(ds2)\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/torch/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d59ee647cd694cba88b02650921963d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.5762, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6635, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8314, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7758, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0593, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5923, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1008, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.9025, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9038, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5786, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.4628, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7163, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.4930, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1504, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0087, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.2622, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3853, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(1.4544, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5743, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.9492, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2635, device='mps:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/torch/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:52: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from argparse import ArgumentParser\n",
    "from pretrain import T5FineTuner, LoggingCallback,set_seed\n",
    "import pytorch_lightning as pl\n",
    "import easydict\n",
    "from transformers import AutoTokenizer\n",
    "from torchinfo import summary\n",
    "import torch\n",
    "\n",
    "args = easydict.EasyDict({\n",
    "        \"max_input_length\": 512,\n",
    "        \"max_output_length\": 512,\n",
    "        \"num_train_epochs\":100,\n",
    "        \"output_dir\": 't5_pretraining',\n",
    "        \"train_batch_size\": 2,\n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"model_name_or_path\":'lcw99/t5-base-korean-text-summary',\n",
    "        \"tokenizer_name_or_path\":'lcw99/t5-base-korean-text-summary',\n",
    "        \"freeze_encoder\":False,\n",
    "        \"freeze_embeds\":False,\n",
    "        'weight_decay':0.0,\n",
    "        'adam_epsilon':1e-8,\n",
    "        'warmup_steps':0,\n",
    "        'train_batch_size':2,\n",
    "        'eval_batch_size':2,\n",
    "        'num_train_epochs':100,\n",
    "        'gradient_accumulation_steps':1,\n",
    "        'n_gpu':1,\n",
    "        'resume_from_checkpoint':None, \n",
    "        # 'val_check_interval' : 10,\n",
    "        'check_val_every_n_epoch':2,\n",
    "        'n_val':4,\n",
    "        'val_percent_check': 5,\n",
    "        'n_train':250,\n",
    "        'n_test':-1,\n",
    "        'early_stop_callback':False,\n",
    "        'fp_16':False, # if you want to enable 16-bit training then install apex and set this to true\n",
    "        'opt_level':'O1', # you can find out more on optimisation levels here https://nvidia.github.io/apex/amp.html#opt-levels-and-properties\n",
    "        'max_grad_norm':0.5, # if you enable 16-bit training then set this to a sensible value, 0.5 is a good default\n",
    "        'seed':42,\n",
    "\n",
    "})\n",
    "\n",
    "## Define Checkpoint function\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    dirpath=args.output_dir,filename='modelcheckpoint')\n",
    "\n",
    "## If resuming from checkpoint, add an arg resume_from_checkpoint\n",
    "train_params = dict(\n",
    "    accumulate_grad_batches=args.gradient_accumulation_steps,\n",
    "    accelerator=\"gpu\",\n",
    "    inference_mode=False,\n",
    "    # gpus=args.n_gpu,\n",
    "    max_epochs=args.num_train_epochs,\n",
    "    precision= 16 if args.fp_16 else 32,\n",
    "    check_val_every_n_epoch=args.check_val_every_n_epoch,\n",
    "    callbacks=[LoggingCallback(),checkpoint_callback]\n",
    ")\n",
    "\n",
    "set_seed(42)\n",
    "model = T5FineTuner(args)\n",
    "tokenizer=AutoTokenizer.from_pretrained(args.tokenizer_name_or_path)\n",
    "print(summary(model,input_data={'input_ids': torch.tensor([[111]*2048]),\n",
    "'decoder_input_ids': torch.tensor([[111]*1]),\n",
    "'attention_mask': torch.tensor([[1]*2048]),\n",
    "'decoder_attention_mask': torch.tensor([[1]*1]),\n",
    "}))\n",
    "trainer = pl.Trainer(**train_params)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
