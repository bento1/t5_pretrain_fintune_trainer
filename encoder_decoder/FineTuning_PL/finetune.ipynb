{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from argparse import ArgumentParser\n",
    "from finetune_model import FintuneModel, LoggingCallback,set_seed\n",
    "from news_dataset import NewsSummaryDataModule\n",
    "import pytorch_lightning as pl\n",
    "import easydict\n",
    "from transformers import AutoTokenizer\n",
    "from torchinfo import summary\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "args = easydict.EasyDict({\n",
    "        \"max_input_length\": 512,\n",
    "        \"max_output_length\": 512,\n",
    "        \"num_train_epochs\":100,\n",
    "        \"output_dir\": 't5_pretraining',\n",
    "        \"train_batch_size\": 2,\n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"model_name_or_path\":'lcw99/t5-base-korean-text-summary',\n",
    "        \"tokenizer_name_or_path\":'lcw99/t5-base-korean-text-summary',\n",
    "        \"freeze_encoder\":False,\n",
    "        \"freeze_embeds\":False,\n",
    "        'weight_decay':0.0,\n",
    "        'adam_epsilon':1e-8,\n",
    "        'warmup_steps':0,\n",
    "        'train_batch_size':2,\n",
    "        'eval_batch_size':2,\n",
    "        'num_train_epochs':100,\n",
    "        'gradient_accumulation_steps':1,\n",
    "        'n_gpu':1,\n",
    "        'resume_from_checkpoint':None, \n",
    "        # 'val_check_interval' : 10,\n",
    "        'check_val_every_n_epoch':2,\n",
    "        'n_val':4,\n",
    "        'val_percent_check': 5,\n",
    "        'n_train':50,\n",
    "        'n_test':-1,\n",
    "        'early_stop_callback':False,\n",
    "        'fp_16':False, # if you want to enable 16-bit training then install apex and set this to true\n",
    "        'opt_level':'O1', # you can find out more on optimisation levels here https://nvidia.github.io/apex/amp.html#opt-levels-and-properties\n",
    "        'max_grad_norm':0.5, # if you enable 16-bit training then set this to a sensible value, 0.5 is a good default\n",
    "        'seed':42,\n",
    "\n",
    "})\n",
    "\n",
    "## Define Checkpoint function\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    dirpath=args.output_dir,filename='modelcheckpoint')\n",
    "\n",
    "## If resuming from checkpoint, add an arg resume_from_checkpoint\n",
    "train_params = dict(\n",
    "    accumulate_grad_batches=args.gradient_accumulation_steps,\n",
    "    accelerator=\"gpu\",\n",
    "    inference_mode=False,\n",
    "    # gpus=args.n_gpu,\n",
    "    max_epochs=args.num_train_epochs,\n",
    "    precision= 16 if args.fp_16 else 32,\n",
    "    # amp_level=args.opt_level,\n",
    "    # gradient_clip_val=args.max_grad_norm,\n",
    "    # checkpoint_callback=checkpoint_callback,\n",
    "    # val_check_interval=args.val_check_interval,\n",
    "    check_val_every_n_epoch=args.check_val_every_n_epoch,\n",
    "    callbacks=[LoggingCallback(),checkpoint_callback]\n",
    ")\n",
    "\n",
    "set_seed(42)\n",
    "model = FintuneModel(args)\n",
    "tokenizer=AutoTokenizer.from_pretrained(args.tokenizer_name_or_path)\n",
    "\n",
    "df = pd.read_csv(\"/Users/dongunyun/study/datascience/encoder_decoder/dataset/news_summary.csv\")\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "news_summary_module = NewsSummaryDataModule(train_df, val_df, test_df, args.train_batch_size, tokenizer, args.max_input_length, args.max_output_length)\n",
    "news_summary_module.setup()\n",
    "trainer = pl.Trainer(**train_params)\n",
    "trainer.fit(model,news_summary_module)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
